training:
  batch_size: 8
  max_length: 128
  epochs: 3
  d_model: 512
  n_heads: 8
  vocab_size: 37000
  shuffle: True
  ffn_hidden: 2048
  n_layers: 6
  init_lr: 3e-5
  weight_decay: 0.01
  adam_eps: 1e-8
  lr_scheduler: 
    factor: 0.5
    patience: 3
  data_percentage: 0.001